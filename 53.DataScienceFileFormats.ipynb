{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python file formats for data science\n",
    "\n",
    "## pandas\n",
    "\n",
    "\n",
    "## Arrow\n",
    "\n",
    "Standard for high-performance in-memory columnar data structures and IO.\n",
    "\n",
    "## Feather\n",
    "\n",
    "## Parquet\n",
    "\n",
    "Parquet is columnar file format designed for small size and IO efficiency, Arrow is an in-memory columnar container ideal as a transport layer to and from Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "arrow_table = pq.read_table('path-to-data/0.parquet')\n",
    "df = arrow_table.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "## Ibis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We'll be writing more documentation and blog posts about Parquet in the coming months.\n",
    "\n",
    "Continuum Analytics recently started developing a separate Parquet implementation for Python that uses Numba for accelerating encoding and decoding routines. I'm glad to see more Python developers working on these problems.\n",
    "\n",
    "Feather file format\n",
    "Earlier this year, I worked with Hadley Wickham to design and deliver the Feather file format for R and Python. Feather uses Apache Arrow's columnar representation and sports a simple metadata specification that can handle the main R and Python data types.\n",
    "\n",
    "As time has passed, as one might expect, quite a bit of code overlap has developed between Feather's C++ and Python components and Apache Arrow's respective C++ and Python libraries. To address this, I'm planning to merge the Feather implementation into the Arrow codebase, which will enable me to provide better performance and new features to Feather users.\n",
    "\n",
    "Improving PySpark\n",
    "PySpark, the Python API for Apache Spark, has well known performance issues (compared with Scala equivalents) around data serialization (Spark's DataFrame.toPandas and sqlContext.createDataFrame) and relatedly UDF evaluation (rdd.map, rdd.mapPartition, or sqlContext.registerFunction).\n",
    "\n",
    "In line with the above work on fast columnar IO for Python and the JVM using Arrow, some of my Two Sigma colleagues and I are collaborating with IBM and the Spark community to accelerate PySpark using these new technologies. If you would like to get involved with this, please let me know.\n",
    "\n",
    "The PySpark-Arrow work is progressing well, and we should have some interesting updates to share in February at the Spark Summit East conference.\n",
    "\n",
    "Update on Ibis\n",
    "Last, but not least, I am still maintaining Ibis and helping its users how I can. I'm proud of the level of deep SQL semantic support that Ibis provides its users. You can write very complex queries with pandas-like Python code that is composable and reusable.\n",
    "\n",
    "As pandas 2.0 progresses, I am interested in building an in-memory backend for Ibis. I had thought about doing this in the past, but decided it would be better to wait.\n",
    "\n",
    "Â© 2017 Wes McKinney  Back to top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
